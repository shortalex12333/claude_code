# LinkedIn A/B Testing Protocol
**Program:** LKD-MKT-2025-Q4
**Created:** November 2025
**Purpose:** Systematic testing framework to optimize content performance
**Statistical Rigor:** Built for meaningful results, not vanity metrics

---

## TESTING PHILOSOPHY

**Core Principle:** Test one variable at a time, measure rigorously, implement learnings.

**Why A/B Test:**
- LinkedIn algorithm changes constantly
- Audience preferences evolve
- What works for others may not work for you
- Assumptions < Data

**What NOT to do:**
- ❌ Test multiple variables simultaneously (can't isolate cause)
- ❌ Compare apples to oranges (different content topics)
- ❌ Declare winners too early (need statistical significance)
- ❌ Ignore learnings (testing without implementation is waste)

---

## 12 CORE A/B TEST HYPOTHESES

### TEST 1: Hook Type Performance

**Hypothesis:** Question hooks drive higher engagement than statistic hooks

**Variable:** Hook type (first 120 characters)

**Constant:** Everything else (same topic, length, time, pillar)

**Test Design:**
- **Version A:** Question hook
  *"Why is it so hard to get clients to respond to proposals?"*

- **Version B:** Statistic hook
  *"73% of proposals go unanswered. Here's why:"*

**Success Metric:** Engagement rate (primary), Comments (secondary)

**Duration:** Both posts same day of week, 2 weeks apart

**Sample Size:** Minimum 5,000 impressions each

**Statistical Significance:** >20% difference to declare winner

**Expected Outcome:** Question hooks drive 1.5x more comments (based on research)

---

### TEST 2: Post Length Optimization

**Hypothesis:** 800-1200 character posts outperform both shorter and longer

**Variable:** Character count

**Constant:** Same hook type, topic, pillar, posting time

**Test Design:**
- **Version A:** Short post (400-600 characters)
- **Version B:** Medium post (800-1200 characters)
- **Version C:** Long post (1400-1600 characters)

**Success Metric:** Dwell time proxy (engagement rate + save rate)

**Duration:** 3 consecutive Tuesdays

**Sample Size:** Minimum 5,000 impressions each

**Statistical Significance:** >15% difference

**Expected Outcome:** Medium-length posts win (algorithm sweet spot per research)

---

### TEST 3: Posting Time Analysis

**Hypothesis:** Tuesday 8-10am outperforms afternoon/evening

**Variable:** Posting time

**Constant:** Same content (can repost same exact post at different times)

**Test Design:**
- **Version A:** Tuesday 8:30am
- **Version B:** Tuesday 12:30pm (lunch time)
- **Version C:** Tuesday 5:30pm (after work)

**Success Metric:** Engagement rate in first 3 hours

**Duration:** 3 consecutive weeks (same day, different times)

**Sample Size:** Minimum 5,000 impressions each

**Statistical Significance:** >25% difference

**Expected Outcome:** Morning post wins (per algorithm research - 6.9% vs 5.8% avg)

---

### TEST 4: Content Format Comparison

**Hypothesis:** Carousel posts drive higher save rate than text-only

**Variable:** Format (text vs. carousel)

**Constant:** Same content information, just different delivery

**Test Design:**
- **Version A:** Text-only post (800 characters, formatted with breaks)
- **Version B:** Carousel PDF (5-7 slides with same information)

**Success Metric:** Save rate (primary), Engagement rate (secondary)

**Duration:** Same week, different days (Tuesday vs Thursday)

**Sample Size:** Minimum 5,000 impressions each

**Statistical Significance:** >30% difference in saves

**Expected Outcome:** Carousel wins for saves (3x research suggests), text wins for comments

---

### TEST 5: CTA Effectiveness

**Hypothesis:** Specific questions outperform generic "What do you think?"

**Variable:** Call-to-action (last line of post)

**Constant:** Same post content, only CTA differs

**Test Design:**
- **Version A:** Generic CTA
  *"What are your thoughts?"*

- **Version B:** Specific question CTA
  *"Which of these tactics have you tried?"*

- **Version C:** Invitation CTA
  *"Drop your biggest challenge in the comments and I'll share my approach"*

**Success Metric:** Comment quantity and quality

**Duration:** 3 weeks, same day

**Sample Size:** Minimum 5,000 impressions each

**Statistical Significance:** >20% difference in comments

**Expected Outcome:** Specific question or invitation wins (easier to respond to)

---

### TEST 6: Pillar Performance by Day

**Hypothesis:** Tactical content (P1) performs better early week, Stories (P4) better late week

**Variable:** Pillar type by day of week

**Constant:** Quality of content, posting time

**Test Design:**
- **Week 1:** P1 (Tactical) on Tuesday, P4 (Story) on Friday
- **Week 2:** P1 (Tactical) on Friday, P4 (Story) on Tuesday

**Success Metric:** Engagement rate relative to average

**Duration:** 2 weeks

**Sample Size:** 4 posts total

**Statistical Significance:** >15% difference

**Expected Outcome:** Tactical Tuesday wins, Story Friday wins (audience mindset differs)

---

### TEST 7: Vulnerability Level Impact

**Hypothesis:** Highly vulnerable posts drive higher engagement than polished posts

**Variable:** Vulnerability/polish level

**Constant:** Same story content, different presentation

**Test Design:**
- **Version A:** Polished story
  *"We faced a challenge and overcame it with strategy X"*

- **Version B:** Vulnerable story
  *"I screwed up. Here's what happened and how I feel about it:"*

**Success Metric:** Comment depth (long comments), emotional reactions, DMs

**Duration:** 2 weeks apart

**Sample Size:** Minimum 5,000 impressions each

**Statistical Significance:** >20% difference in engagement

**Expected Outcome:** Vulnerable version wins (authenticity resonates per research)

---

### TEST 8: Data Presentation Style

**Hypothesis:** Visual data (described) outperforms paragraph data

**Variable:** Data presentation format

**Constant:** Same data/insights

**Test Design:**
- **Version A:** Paragraph style
  *"I found that 73% of companies X, while 27% do Y. This means..."*

- **Version B:** Visual/Structured style
  *"The data:*
  *→ 73% of companies X*
  *→ 27% of companies Y*
  *What this means:..."*

**Success Metric:** Saves and shares (data utility)

**Duration:** Same week, different days

**Sample Size:** Minimum 5,000 impressions each

**Statistical Significance:** >15% difference

**Expected Outcome:** Structured wins (scannability drives saves)

---

### TEST 9: Hashtag Quantity Test

**Hypothesis:** 2-3 specific hashtags outperform 5-10 generic hashtags

**Variable:** Number and type of hashtags

**Constant:** Same post content

**Test Design:**
- **Version A:** 2-3 specific hashtags (#B2BSales #SaaSMarketing)
- **Version B:** 5-10 hashtags (mix of generic and specific)
- **Version C:** No hashtags

**Success Metric:** Reach beyond network (new profile views)

**Duration:** 3 weeks, same day

**Sample Size:** Minimum 5,000 impressions each

**Statistical Significance:** >10% difference in reach

**Expected Outcome:** 2-3 specific hashtags win (algorithm deprioritizes over-hashtagging)

---

### TEST 10: Opinion Strength Test

**Hypothesis:** Strong opinions drive higher engagement than nuanced takes

**Variable:** Opinion strength/boldness

**Constant:** Same topic and pillar

**Test Design:**
- **Version A:** Bold/Controversial
  *"Cold calling is dead. Anyone still doing it is wasting time."*

- **Version B:** Nuanced/Balanced
  *"Cold calling effectiveness has declined 40%. Here's when it still works:"*

**Success Metric:** Comments (especially debate), engagement rate

**Duration:** 2 weeks apart

**Sample Size:** Minimum 5,000 impressions each

**Statistical Significance:** >25% difference in comments

**Expected Outcome:** Bold wins for comments (sparks debate), nuanced wins for saves (more useful)

---

### TEST 11: Personal vs Company Page

**Hypothesis:** Same content performs better on personal page than company page

**Variable:** Page type (personal vs company)

**Constant:** Exact same post content

**Test Design:**
- **Version A:** Posted from personal profile
- **Version B:** Posted from company page (same day, same time)

**Success Metric:** Engagement rate, reach

**Duration:** Can test simultaneously

**Sample Size:** Minimum 5,000 impressions each

**Statistical Significance:** >50% difference (expect large gap)

**Expected Outcome:** Personal profile wins 2-3x (algorithm favors personal)

---

### TEST 12: Line Break Frequency

**Hypothesis:** Line breaks every 1-2 sentences outperform dense paragraphs

**Variable:** Line break frequency

**Constant:** Same content, word-for-word

**Test Design:**
- **Version A:** Dense paragraphs (3-4 sentences per paragraph)
- **Version B:** Frequent breaks (1-2 sentences per paragraph)
- **Version C:** Excessive breaks (1 sentence per line)

**Success Metric:** Dwell time proxy (engagement rate)

**Duration:** 3 weeks, same day

**Sample Size:** Minimum 5,000 impressions each

**Statistical Significance:** >10% difference

**Expected Outcome:** Frequent breaks win (readability = dwell time)

---

## TESTING METHODOLOGY

### Phase 1: Design (Week 1)
- [ ] Choose hypothesis to test
- [ ] Define variable and constants
- [ ] Create test versions (A vs B vs C)
- [ ] Determine success metrics
- [ ] Set statistical significance threshold
- [ ] Schedule posts in calendar

### Phase 2: Execute (Week 2-4)
- [ ] Post version A (week 1)
- [ ] Track performance for 48 hours
- [ ] Post version B (week 2, same day/time)
- [ ] Track performance for 48 hours
- [ ] Post version C if applicable (week 3)
- [ ] Compile data in tracking sheet

### Phase 3: Analyze (Week 5)
- [ ] Compare metrics side-by-side
- [ ] Calculate percentage difference
- [ ] Determine statistical significance
- [ ] Document insights
- [ ] Declare winner (or "inconclusive")

### Phase 4: Implement (Week 6+)
- [ ] Update content guidelines based on winner
- [ ] Apply learnings to future posts
- [ ] Share learnings with team
- [ ] Plan next test

---

## STATISTICAL SIGNIFICANCE CALCULATOR

### Sample Formula:
```
Difference % = ((Version B - Version A) / Version A) × 100

Significant? = Difference % > Significance Threshold AND Sample Size > 5,000
```

### Example:
- **Version A:** 5.2% engagement rate (8,000 impressions)
- **Version B:** 6.8% engagement rate (8,200 impressions)
- **Difference:** ((6.8 - 5.2) / 5.2) × 100 = **30.7% improvement**
- **Significance Threshold:** 20%
- **Result:** Version B wins ✅

---

## TRACKING TEMPLATE

| Test # | Hypothesis | Version | Post Date | Impressions | Engagement % | Comments | Saves | Winner | Insight |
|--------|-----------|---------|-----------|-------------|--------------|----------|-------|--------|---------|
| 1 | Question vs Stat hook | A (Question) | 11/5/25 | 8,200 | 6.8% | 127 | 34 | A | Questions drive 31% more comments |
| 1 | Question vs Stat hook | B (Stat) | 11/12/25 | 8,100 | 5.2% | 97 | 41 | - | Stats drive more saves |
| 2 | Post length | A (Short) | 11/19/25 | 7,800 | 4.1% | 68 | 22 | - | - |
| 2 | Post length | B (Medium) | 11/26/25 | 8,300 | 6.3% | 104 | 45 | B | Medium length wins |

---

## TEST PRIORITIZATION

**Run tests in this order (highest impact → lowest):**

### Priority 1 (Month 1): Foundation Tests
1. **Test 3:** Posting time (affects every post)
2. **Test 2:** Post length (core formatting)
3. **Test 12:** Line breaks (readability)

**Impact:** These affect ALL future content, so test first

---

### Priority 2 (Month 2): Hook & Structure Tests
4. **Test 1:** Hook types
5. **Test 5:** CTA effectiveness
6. **Test 8:** Data presentation

**Impact:** Optimizes engagement drivers

---

### Priority 3 (Month 3): Format & Strategy Tests
7. **Test 4:** Content format (text vs carousel)
8. **Test 6:** Pillar timing
9. **Test 11:** Personal vs company page

**Impact:** Strategic decisions

---

### Priority 4 (Ongoing): Advanced Tests
10. **Test 7:** Vulnerability level
11. **Test 9:** Hashtag strategy
12. **Test 10:** Opinion strength

**Impact:** Fine-tuning and optimization

---

## COMMON TESTING MISTAKES

### ❌ Mistake 1: Changing Multiple Variables
**Problem:** Can't isolate what caused difference
**Solution:** Change ONE variable per test

### ❌ Mistake 2: Testing Too Early
**Problem:** Not enough data to declare winner
**Solution:** Wait for 5,000+ impressions, 48 hours minimum

### ❌ Mistake 3: Ignoring Context
**Problem:** External factors skew results (holidays, news events)
**Solution:** Note any unusual circumstances, retest if suspicious

### ❌ Mistake 4: Not Implementing Learnings
**Problem:** Testing without action is wasted effort
**Solution:** Update guidelines immediately after test concludes

### ❌ Mistake 5: Confirmation Bias
**Problem:** Only noticing data that confirms hypothesis
**Solution:** Let data speak, be willing to be wrong

---

## ADVANCED TESTING: MULTI-VARIATE

Once you've run 12 core tests, you can combine learnings:

### Example Multi-Variate Test:
**Hypothesis:** Combining all winning elements creates super-post

**Test:**
- **Version A:** Current best practices (from tests 1-12)
- **Version B:** One sub-optimal element reintroduced
- **Goal:** Validate that learnings compound

---

## MONTHLY TESTING SCHEDULE

### Week 1:
- Design test
- Create versions
- Schedule posts

### Week 2:
- Post Version A
- Monitor performance

### Week 3:
- Post Version B
- Monitor performance

### Week 4:
- Analyze results
- Document learnings
- Implement changes
- Plan next test

**Continuous cycle: Always be testing**

---

## REPORTING TEMPLATE

### Test #[X]: [Hypothesis Name]

**Hypothesis:** [Clear statement]

**Variables Tested:**
- Version A: [Description]
- Version B: [Description]

**Results:**
| Metric | Version A | Version B | Difference | Winner |
|--------|-----------|-----------|------------|--------|
| Impressions | 8,200 | 8,100 | -1.2% | Tie |
| Engagement % | 6.8% | 5.2% | +30.7% | A ✅ |
| Comments | 127 | 97 | +30.9% | A ✅ |
| Saves | 34 | 41 | -17.1% | B |

**Conclusion:** Version A (Question hooks) drives significantly higher engagement and comments. Version B (Stat hooks) drives slightly more saves.

**Recommendation:** Use question hooks for engagement-focused posts (P1, P4, P6). Use stat hooks for save-focused posts (P2, P5).

**Implementation:** Updated Hook Library to note this distinction. Will use question hooks 70% of time, stat hooks 30%.

**Next Test:** Test #2 (Post length optimization)

---

**Document Status:** Complete ✅

**Success Criteria:**
- [ ] Run minimum 1 test per month
- [ ] Achieve 80% completion rate on planned tests
- [ ] Implement 100% of validated learnings
- [ ] Share insights with team after each test

*A/B testing is not optional. It's the difference between guessing and knowing what works for YOUR specific audience on LinkedIn.*
